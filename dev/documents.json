[{"doctype":"documentation","id":"references/DataLoaders.obsslices","title":"obsslices","text":" obsslices ( batch ,  batchdim   =  BatchDimLast ( ) ) Iterate over views of all observations in a  batch . batch  can be a batched array, a tuple of batches, or a dict of batches . batch   =  rand ( 10 ,  10 ,  4 )  # batch size is 4\n iter   =  obsslices ( batch ,  BatchDimLast ( ) )\n @assert    size ( first ( iter ) )  ==  ( 10 ,  10 )\n\n iter2   =  obsslices ( batch ,  BatchDimFirst ( ) )\n @assert    size ( first ( iter ) )  ==  ( 10 ,  4 )"},{"doctype":"documentation","id":"references/DataLoaders.BatchDim","title":"BatchDim","text":""},{"doctype":"document","id":"documents/docs/status.md","title":"TODOs","text":" TODOs Features make compatible with  ObsDim s Documentation document data iterators better document port  PyTorch custom dataset tutorial"},{"doctype":"document","id":"documents/README.md","title":"DataLoaders.jl","text":" DataLoaders . jl Documentation (latest) A Julia package implementing performant data loading for deep learning on out - of - memory datasets that .  Works like PyTorch ’ s  DataLoader . What does it do? Uses multi - threading to load data in parallel while keeping the primary thread free for the training loop Handles batching and  collating Is simple to  extend  for custom datasets Integrates well with other packages in the  ecosystem Allows for  inplace loading  to reduce memory load When should you use it? You have a dataset that does not fit into memory You want to reduce the time your training loop is waiting for the next batch of data How do you use it? Install like any other Julia package using the package manager (see  setup ): ] add   DataLoaders After installation, import it, create a  DataLoader  from a dataset and batch size, and iterate over it: using   DataLoaders \n# 10.000 observations of inputs with 128 features and one target feature\n data   =  ( rand ( 128 ,  10000 ) ,  rand ( 1 ,  10000 ) )\n dataloader   =  DataLoader ( data ,  16 )\n\n for   ( xs ,  ys )  in  dataloader \n     @assert    size ( xs )  ==  ( 128 ,  16 )\n     @assert    size ( ys )  ==  ( 1 ,  16 )\n end Next, you may want to read What datasets you can use it with How it compares to PyTorch ’ s data loader"},{"doctype":"documentation","id":"references/DataLoaders.PoolState","title":"PoolState","text":""},{"doctype":"documentation","id":"references/DataLoaders.Done","title":"Done","text":""},{"doctype":"document","id":"documents/docs/datacontainers.md","title":"Data containers","text":" Data containers Introduction to data containers giving an overview over the kinds of datasets you can use DataLoaders . jl is built to integrate with the further ecosystem and builds on a common interface to support datasets .  We call such a dataset a  data container  and it needs to support the following operations: getobs(data, i)  loads the  i - th observation from a dataset nobs(data)  gives the number of observations in a dataset . Basic data containers The simplest data container is a vector of values: using   DataLoaders \n @show    v   =  rand ( 1 : 10 ,  10 )\n @show    nobs ( v )\n getobs ( v ,  1 ) v = rand(1:10, 10) = [9, 3, 10, 9, 3, 7, 5, 6, 1, 1]\nnobs(v) = 10\n Multi - dimensional arrays also work, with the last dimension treated as the observation dimension: a   =  rand ( 50 ,  50 ,  10 )\n summary ( getobs ( a ,  1 ) )  50×50 Matrix{Float64} You can also group multiple data containers with the same length together by putting them into a  Tuple : data   =  ( v ,  a )\n getobs ( data ,  1 )  You can pass any data container to  DataLoader  to create an iterator over batches: for   batch   in  DataLoader ( v ,  2 )\n     @assert    size ( batch )  ==  ( 2 , )\n end \n\n for   batch   in  DataLoader ( a ,  2 ) \n     @assert    size ( batch )  ==  ( 50 ,  50 ,  2 )\n end \n\n for   ( vs ,  as )  in  DataLoader ( ( v ,  a ) ,  2 ) \n     @assert    size ( vs )  ==  ( 2 , )\n     @assert    size ( as )  ==  ( 50 ,  50 ,  2 )\n end Out - of - memory data containers Arrays, of course, are kept in memory, so we (1) cannot use them to store larger - than - memory datasets (2) don ’ t need to use multithreading since loading an observation just involves indexing an array which is generally fast . One way to quickly get into the territory of too - large - to - fit in memory is to work with image datasets .  So instead of loading every image of a dataset into an array, we ’ ll implement a data container that stores only the file names of each image .  It will load the image itself only when  getobs  is called .  To do that we ’ ll implement a  struct  that stores a vector of file names, and implement  getobs  and  nobs  for that type . import   DataLoaders . LearnBase :  getobs ,  nobs \n using   Images \n\n struct    ImageDataset \n     files :: Vector { String }\n end \n ImageDataset ( folder :: String )  =  ImageDataset ( readdir ( folder ) )\n\n nobs ( data :: ImageDataset )  =  length ( data . files )\n getobs ( data :: ImageDataset ,  i :: Int )  =  Images . load ( data . files [ i ] ) Now, if we have a folder full of images, we can create a data container and load them quickly into batches as follows: data   =  ImageDataset ( \"path/to/my/images\" )\n for   images   in  DataLoader ( data ,  16 ,  collate   =  false )\n    # Do something\n  end Preprocessing Above we pass the  collate = false  argument because images may be of different sizes that cannot be collated .  See  collate .  In practice, it is common to apply some cropping and resizing to images so that they all have the same size . Threads To use  DataLoaders ’  multi - threading, you need to start Julia with multiple threads .  Check the number of available threads with  Threads.nthreads() ."},{"doctype":"document","id":"documents/docs/inplaceloading.md","title":"Inplace loading","text":" Inplace loading Background on inplace loading of data When loading an observation of a  data container  requires allocating a lot of memory, it is sometimes possible to reuse a previous observation as a buffer to load into .  To do so, the data container you ’ re using, must  implement  getobs! .  To use the buffered loading with this package, pass  buffered = true  to  DataLoader .  This also works for collated batches ."},{"doctype":"documentation","id":"references/DataLoaders.Failed","title":"Failed","text":""},{"doctype":"document","id":"documents/docs/ecosystem.md","title":"Ecosystem","text":" Ecosystem Overview of packages that DataLoaders . jl builds on or that use it . This package is part of an ecosystem of packages providing useful tools for machine learning in Julia .  These compose nicely due to shared interface packages like  LearnBase . jl  and the natural extensibility that Julia ’ s multiple dispatch provides .  DataLoaders . jl works with any package the implements the  data container interface .  This means you can easily drop it in to an existing workflow or use the functionality of other packages to work with DataLoaders . jl more effectively . The most important package for manipulating data containers is  MLDataPattern . jl  which provides a large set of tools for transforming and composing data containers .  Some examples are given here:  Shuffling, subsetting, splitting MLDatasets . jl  makes it easy to load common benchmark datasets as data containers . A package that makes heavy use of DataLoaders . jl to train large deep learning models is  FastAI . jl .  It also provides many easy - to - load data containers for larger computer vision, tabular, and NLP datasets ."},{"doctype":"documentation","id":"references/DataLoaders","title":"DataLoaders","text":""},{"doctype":"document","id":"documents/docs/setup.md","title":"Installation","text":" Installation Julia DataLoaders . jl is a package for the  Julia Programming Language .  To use the package you need to install Julia, which you can download  here . DataLoaders . jl Julia has a built - in package manager which is used to install packages .  Running the installed  julia  command launches an interactive session .  To install DataLoaders . jl, run the following command: using   Pkg;   Pkg . add ( \"DataLoaders\" ) Enabling multi - threading To make use of multi - threaded data loading, you need to start Julia with multiple threads .  If starting the  julia  executable yourself, you can pass a  -t <nthreads>  argument or set the environment variable  JULIA_NUM_THREADS  beforehand .  To check that you have multiple threads available to you, run: julia >  Threads . nthreads ( )\n 12 If you ’ re running Julia in a Jupyter notebook, see  IJulia . jl ’ s documentation ."},{"doctype":"documentation","id":"references/DataLoaders.obsslice","title":"obsslice","text":""},{"doctype":"documentation","id":"references/DataLoaders.BatchDimFirst","title":"BatchDimFirst","text":""},{"doctype":"documentation","id":"references/DataLoaders._batchsize","title":"_batchsize","text":""},{"doctype":"document","id":"documents/docs/collate.md","title":"Collating","text":" Collating Collating refers to combining a batch of observations so that the arrays in individual observations are stacked together .  As an example, consider a dataset with 100 observations, each with 2 features . data   =  rand ( 2 ,  100 )  # observation dimension is last by default   We can collect a batch of 4 observations into a vector as follows: using   DataLoaders \n batch   =  [ getobs ( data ,  i )  for   i   in  1 : 4 ]  Many machine learning models, however, expect an input in a  collated  format: instead of a nested vector of vectors, we need a single ND - array .  DataLoaders . jl provides the  collate  function for this: DataLoaders . collate ( batch )  As you can see, the batch dimension is the last one by default . Nested observations The above case only shows how to collate observations that each consist of a single array .  In practice, however, observations will often consist of multiple variables like input features and a target variable .  For example, we could have an integer indicating the class of an input sample . inputs   =  rand ( 2 ,  100 )\n targets   =  rand ( 1 : 10 ,  100 )\n data   =  ( inputs ,  targets )\n batch   =  [ getobs ( data ,  i )  for   i   in  1 : 4 ]  Collating here also works, by keeping the tuple structure and collating each element separately: DataLoaders . collate ( batch )  This is also implemented for  NamedTuple s and  Dict s .  You can also collate nested structures, e . g .  a  Tuple  of  Dict s and the structure is preserved .  This also works when using  inplace loading ."},{"doctype":"documentation","id":"references/DataLoaders.collate","title":"collate","text":" collate ( samples ) Collates a vector of samples into a single batch .  See  collating ."},{"doctype":"documentation","id":"references/DataLoaders.BatchDimLast","title":"BatchDimLast","text":""},{"doctype":"documentation","id":"references/DataLoaders.PoolFailedException","title":"PoolFailedException","text":""},{"doctype":"documentation","id":"references/DataLoaders.@pack_BatchViewCollated","title":"@pack_BatchViewCollated","text":""},{"doctype":"documentation","id":"references/DataLoaders.copyrec!","title":"copyrec!","text":""},{"doctype":"documentation","id":"references/DataLoaders.BatchViewCollated","title":"BatchViewCollated","text":" BatchViewCollated ( data ,  size;   droplast   =  false ) A batch view of container  data  with collated batches of size  size ."},{"doctype":"documentation","id":"references/DataLoaders.@unpack_BatchViewCollated","title":"@unpack_BatchViewCollated","text":""},{"doctype":"documentation","id":"references/DataLoaders.DataLoader","title":"DataLoader","text":" DataLoader (\n     data ,\n     batchsize   =  1;\n     partial   =  true ,\n     collate   =  true ,\n     buffered   =  collate ,\n     parallel   =  Threads . nthreads ( )  >  1 ,\n     useprimary   =  false ,\n ) Create an efficient iterator of batches over  data container   data . Arguments Positional data : A data container supporting the  LearnBase  data access pattern batchsize = 1 : Number of samples to batch together .  Disable batching by setting to  nothing . Keyword partial::Bool = true : Whether to include the last batch when  nobs(dataset)  is not divisible by  batchsize .   true  ensures all batches have the same size, but some samples might be dropped . buffered::Bool = collate : If  buffered  is  true , loads data inplace using  getobs! .  See  Data containers  for details on buffered loading . parallel::Bool = Threads.nthreads() > 1) : Whether to load data in parallel, keeping the primary thread is .  Default is  true  if more than one thread is available . useprimary::Bool = false : If  false , keep the main thread free when loading data in parallel .  Is ignored if  parallel  is  false . Examples Creating a data loader with batch size 16 and iterating over it: data   =  ( rand ( 128 ,  10000 ) ,  rand ( 1 ,  10000 ) )\n dataloader   =  DataLoader ( data ,  16 )\n\n for   ( xs ,  ys )  in  dataloader \n  end Creating a data loader that  uses buffers  to load batches: data   =  rand ( 100 ,  64 )\n dataloader   =  DataLoader ( data ,  16 ,  buffered = true )\n\n size ( first ( dataloader ) )  ==  ( 100 ,  16 ) Turning off  collating : dataloader   =  DataLoader ( data ,  16 ,  collate = false )\n\n# Batches are a vector of observations\n length ( first ( dataloader ) )  ==  16"},{"doctype":"document","id":"documents/docs/overview.md","title":"Overview","text":" Overview The central function in  DataLoaders  is, of course,  DataLoader . It provides a wrapper around a parallel data iterator,  eachobsparallel Finally,  batchviewcollated  provides a lazy view of collated batches with support for inplace loading ."},{"doctype":"documentation","id":"references/DataLoaders.eachobsparallel","title":"eachobsparallel","text":" eachobsparallel ( data;   useprimary   =  false ,  buffered   =  true ) Parallel data iterator for data container  data .  Loads data on all available threads (except the first if  useprimary  is  false ) . If  buffered  is  true , uses  getobs!  to load samples inplace . See also  MLDataPattern.eachobs Order eachobsparallel  does not guarantee that the samples are returned in the correct order ."},{"doctype":"documentation","id":"references/DataLoaders.run","title":"run","text":""},{"doctype":"documentation","id":"references/DataLoaders.BufferGetObsParallel","title":"BufferGetObsParallel","text":" BufferGetObsParallel ( data;   useprimary   =  false ) Like  MLDataPattern.BufferGetObs  but preloads observations into a buffer ring with multi - threaded workers ."},{"doctype":"documentation","id":"references/DataLoaders.batchindices","title":"batchindices","text":" batchindices ( n ,  size ,  i ) Get the indices of batch  i  with batch size  size  of a collection with  n  elements . Might be a partial batch if  i  is the last batch and n  is not divisible by  size ."},{"doctype":"document","id":"documents/docs/interface.md","title":"Data container interface","text":" Data container interface Reference for implementing the data container interface .  See  data containers  for an introduction . To implement the data container interface for a custom type  T , you must implement two functions: LearnBase.getobs(data::T, i::Int)  loads the  i - th observation LearnBase.nobs(data::T, i::Int)::Int  gives the number of observations in a data container You can  optionally  also implement: LearnBase.getobs!(buf, data::T, i::Int) : loads the  i - th observation into the preallocated buffer  buf . See  the MLDataPattern . jl documentation  for a comprehensive discussion of and reference for data containers . Extending functions To define a method for the above functions, you need to import the functions explicitly .  You can do this without installing  LearnBase  by running: import   DataLoaders . LearnBase :  getobs ,  nobs ,  getobs!"},{"doctype":"document","id":"documents/docs/quickstartpytorch.md","title":"Comparison to PyTorch","text":" Comparison to PyTorch This package is inspired by Pytorch ’ s  torch.utils.data.DataLoader  and works a lot like it .  The basic usage for both is  DataLoader(dataset, batchsize) , but for other use cases there are some differences . The most important things are: DataLoaders . jl supports only map - style datasets at the moment It uses thread - based parallelism instead of process - based parallelism Detailed comparison Let ’ s go through every argument to  torch.utils.data.DataLoader  and have a look at similarities and differences .  See  DataLoader  for a full list of its arguments . dataset : This package currently only supports map - style datasets which work similarly to Python ’ s, but instead of implementing  __getindex__  and  __len__ , you ’ d implement  LearnBase.getobs  and  nobs .   More info here . batch_size = 1 : If not specified otherwise, the default batch size is 1 for both packages .  In DataLoaders . jl, you can additionally pass in  nothing  to turn off batching . shuffle = false : This package ’ s  DataLoader  does  not  support this argument .  Shuffling should be applied to the dataset beforehand .  See  working with data containers . collate_fn : DataLoaders . jl collates batches by default unless  collate = false  is passed .  A custom collate function is not supported, but you can extend  DataLoaders.collate  for custom data types for the same effect . drop_last = False .  DataLoaders . jl has the same behavior of returning a partial batch by default, but the keyword argument is  partial = false  with  partial = not drop_last . prefetch_factor : This cannot be customized currently .  The default behavior for DataLoaders . jl is for every thread to be preloading one batch . pin_memory : DataLoaders . jl does not interact with the GPU, but you can do this in your data container . num_workers ,  persistent_workers ,  worker_init_fn ,  timeout : Unlike PyTorch, this package does not use multiprocessing, but multithreading which is not practical in Python due to the GIL .  As such these arguments do not apply .  Currently, DataLoaders . jl will use either all threads except the primary one or all threads based on the keyword argument  useprimary = false . sampler ,  batch_sampler ,  generator : This package does not currently support these arguments for customizing the randomness ."},{"doctype":"document","id":"documents/toc.md","title":"toc","text":" Introduction Overview Installation Data containers Shuffling, subsetting, splitting Collating Inplace loading Data container interface Ecosystem Comparison to PyTorch TODOs"},{"doctype":"documentation","id":"references/DataLoaders.WorkerPool","title":"WorkerPool","text":""},{"doctype":"documentation","id":"references/DataLoaders.Running","title":"Running","text":""},{"doctype":"document","id":"documents/docs/howto/workingwith.md","title":"workingwith","text":""},{"doctype":"document","id":"documents/docs/shuffling.md","title":"Shuffling, subsetting, splitting","text":" Shuffling, subsetting, splitting Shuffling your training data every epoch and splitting a dataset into training and validation splits are common practices . While  DataLoaders  itself only provides tools to load your data effectively, using the underlying  MLDataPattern  package makes these things easy . Examples Shuffling using   MLDataPattern :  shuffleobs \n\n data   =  ...\n dataloader   =  DataLoader ( shuffleobs ( data ) ,  batchsize ) Subsetting using   MLDataPattern :  datasubset \n\n data   =  ...\n idxs   =  1 : 1000  # indices to select from dataset\n  \n dataloader   =  DataLoader ( datasubset ( data ,  idxs ) ) ,  batchsize ) Train/validation split using   MLDataPattern :  splitobs \n    \n data   =  ...\n traindata ,  valdata   =  splitobs ( data ,  0.8 )  # 80/20 split\n dataloader   =  DataLoader ( shuffleobs ( data ) ,  batchsize ) But wait, there ’ s more For other dataset operations like weighted sampling, see  this section  in MLDataPattern ’ s documentation ."},{"doctype":"documentation","id":"references/DataLoaders.GetObsParallel","title":"GetObsParallel","text":""},{"doctype":"documentation","id":"references/DataLoaders.RingBuffer","title":"RingBuffer","text":" RingBuffer ( size ,  buf ) A  Channel - like data structure that rotates through size  buffers .   put! s work by mutating one of the buffers: put! ( ringbuffer )  do   buf \n     rand! ( buf )\n end The result can then be  take! n: res   =  take! ( ringbuffer ) Invalidation Only one result is valid at a time !  On the next  take! , the previous result will be reused as a buffer and be mutated by a  put!"},{"doctype":"documentation","id":"references/DataLoaders.batchviewcollated","title":"batchviewcollated","text":""}]